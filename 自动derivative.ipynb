{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMmMTKKjDFZBHQwuwdikjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kally1218/pycharmproject/blob/main/%E8%87%AA%E5%8A%A8derivative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IX-IxSz6GpdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7557e1e9-72f4-49c5-ca9a-a14bee48b6e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "#函数关于列向量x求导, 创建一个向量y 有浮点数的那种 0 ,1, 2 ,3\n",
        "x = torch.arange(4.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#找一个地方存y的梯度(导数),这是 PyTorch 中一个非常重要的步骤，\n",
        "#当你想要计算某个张量（例如 x）相对于另一个张量的梯度（导数）时，就需要这样做。\n",
        "#requires_grad_ 方法当传入 True 时，会原地修改 x 这个张量。\n",
        "#这告诉 PyTorch 需要追踪所有对 x 执行的操作。这种追踪对于后续在反向传播过程中计算梯度是必需的\n",
        "x.requires_grad_(True) #等价于 x= torch.arange(4,0, requires_grad=True)\n",
        "x.grad #默认值是None, 以后这个能访问梯度,梯度就存在这个x.grad里面"
      ],
      "metadata": {
        "id": "lVUG6yH6Oavw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#计算y, 显示y是反向传播\n",
        "y = 2 * torch.dot(x, x) #torch,dot就是向量的內积写法,內积是个scalar哦!!!\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBNBepJxPO6i",
        "outputId": "f980bd2d-4127-4eb0-8f08-9308c5fa3f5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward() #计算y关于x每个分量的梯度\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNiibhUHVNUi",
        "outputId": "f084590e-9688-489a-f4d0-81215c611adf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#验证梯度\n",
        "x.grad == 4 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuJ_vwzaW791",
        "outputId": "715d21e0-e26f-4b22-a35c-a4b839e70b7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#before calculate next gradient, we need to clean previous one\n",
        "x.grad.zero_() #进行一个清除,记住有个下划线!!用于重写内容!!!!\n",
        "#求x的总和\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_CV6nSEXFZm",
        "outputId": "8aa8b0d1-039f-4688-dd2b-deba22ebb93d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#向量的乘法\n",
        "#深度学习中, 调用向量的反向计算的时候,一般, 会计算一个batch training sample中的每个组成部分的损失函数的导数\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "#等价于y.backward(torch.ones(len(x)))\n",
        "y.sum().backward() #一般情况先对y求和,再反向求导\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PaOhebQY-iN",
        "outputId": "3b6657fa-6396-4513-80b3-f05c3551f7c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach() #把y这个向量变成标量,之后要固定网络中的参数时候用, u变成常数了\n",
        "z = u * x #这里u就变成常数了\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u\n"
      ],
      "metadata": {
        "id": "SqrBrdd6Z-RZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d4a0e9-5a4d-448e-d902-bd4846de92f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#用python的控制流,还是能求导\n",
        "#PyTorch 会在 运行时 构建计算图（不是提前构建好）\n",
        "#你写什么操作，它就记录什么操作（即“边写边构建”），操作完成后，\n",
        "#计算图就完整了，可以用 .backward() 自动求导。\n",
        "def f(a):\n",
        "  b = a * 2\n",
        "  while b.norm() < 1000: #norm默认范数2, 开平方取根号的公式\n",
        "    b = b * 2\n",
        "    if b.sum() > 0:\n",
        "      c = b\n",
        "    else:\n",
        "      c = 100 * b\n",
        "    return c\n",
        "\n",
        "a = torch.randn(size=(), requires_grad=True) #a是input的随机数,size里面啥也不写等于标量的)\n",
        "d = f(a)\n",
        "d.backward()\n",
        "\n",
        "a.grad == d / a #右边的式子是因为d的函数是a * 2 *....无限乘,所以,d的导数是个关于k个2的标量\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd4FsR6Uegmd",
        "outputId": "0d55ef4e-73f3-4483-ae1f-4120caf29ea0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}